layout: post
title:  "爬虫小记"
date:   2018-01-02
author: 梁龙飞
tags: 编程
---

最近交接了同事一些爬虫的工作，虽然只是些定时任务，但本着既然涉入了就要了解一番的习惯，对爬虫作了些研究。

## 基本概念

### 模拟登录

这是很多爬虫最难的地方，对于不设防的数据，基本就是些字符解析(比如html,xml)提取入库。但很多网站不登录不能抓数据，所以基于爬与反爬衍生了很多技术。常见用于反爬虫的技术有：

- 为每个请求登录的页面分配一些随机码，作为登录时额外的参数，随机码有可能在前端生成，而前端代码经过混淆。
- 各种验证码识别，更是催生了“极验”这类专门的公司
- 频率控制，爬虫推断，封禁IP

限于本人技术，只能处理上面第一点的情况，这是我写的关于 https://bitbucket.org 这个网站的模拟登录：
(开始是用node写，**但是一直遇到问题，没有解决**，后改成了python) —— 还是要花时间弄清楚node的模拟登录 
```python
# coding:utf-8

import requests
# bs4用于html解析，选择
from bs4 import BeautifulSoup

url = 'https://bitbucket.org/account/signin/'


session = requests.session()


def get_post_headers():
    return {
        'Host': 'bitbucket.org',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:47.0) Gecko/20100101 Firefox/47.0',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Connection': 'keep-alive',
        'Referer': 'https://bitbucket.org/account/signin/'
    }


def get_request_params():
    login_html = session.get(url).text
    soup = BeautifulSoup(login_html, 'html.parser')
    csrf = soup.find(attrs={'name': 'csrfmiddlewaretoken'})['value']
    # 请输入正确的用户名密码
    return dict(csrfmiddlewaretoken=csrf, username='xxx', password='xxx')


def login():
    session.post(url, data=get_request_params(), headers=get_post_headers())
    return session


if  __name__ == "__main__":
    s = login()
    home_page = 'https://bitbucket.org/'
    resp = s.get(home_page, headers='')
    sp = BeautifulSoup(resp.text, 'html.parser')

    # 输出登录后页面的title元素
    print(sp.title)

```

具体的python代码知识在此不赘述。

另外，对于验证码复杂的部分，可以使用人工验证的方式登录获取cookie，交取爬虫接着执行。一般使用selnium + chrome/phantomjs.

### 解析关键信息

web爬虫基本就是解析html，也是本人技术栈的原因，介绍python和node方面的两个库

#### python bs4

bs4在上面的例子里已经使用过了

#### node cheerio

有前端经验的对于使用cheerio会非常爽，因为选择器就是jQuery那一套

#### js渲染

基于前后端分离开发的web页面，html元素都是由js生成的，以http请求返回的页面是没有什么意义的。这时候可以使用phantomjs,它是一个没有UI的浏览器，可以构建执行js的环境，生成完整页面，进而进行抓取。

### 并发控制
爬虫另一个关键的地方，是执行控制，多数使用多个队列来管理，待续。


